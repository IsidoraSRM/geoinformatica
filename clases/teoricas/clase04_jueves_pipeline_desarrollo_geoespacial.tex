\documentclass[10pt,aspectratio=169]{beamer}
\usetheme{metropolis}
\usepackage{FiraSans}
\usefonttheme{professionalfonts}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage[spanish]{babel}
\usepackage{tcolorbox}
\usepackage{ragged2e}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{usachblue}{rgb}{0,0.4,0.7}
\definecolor{usachred}{rgb}{0.8,0,0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\tiny,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=3pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\newcommand{\examplebox}[2]{
\begin{tcolorbox}[colframe=usachblue,colback=blue!5,title=#1]
#2
\end{tcolorbox}
}

\setbeamertemplate{footline}{%
  \begin{beamercolorbox}[wd=\paperwidth,sep=2ex]{footline}%
    \usebeamerfont{structure}\textbf{Geoinformática - Clase 4} \hfill Profesor: Francisco Parra O. \hfill \textbf{Semestre 2, 2025}
  \end{beamercolorbox}%
}

\title{Clase 04: Pipeline de desarrollo geoespacial}
\subtitle{De los datos a la solución en producción}
\author{Profesor: Francisco Parra O.}
\institute{USACH - Ingeniería Civil en Informática}
\date{\today}

\titlegraphic{%
  \begin{tikzpicture}[overlay, remember picture]
    \node[anchor=north east, yshift=0cm] at (current page.north east) {
      \includegraphics[width=1.5cm]{../../logo.jpg}
    };
  \end{tikzpicture}
}

\begin{document}

\maketitle

\begin{frame}{Agenda}
    \tableofcontents[hideallsubsections]
\end{frame}

\section{Flujo de trabajo geoespacial}

\begin{frame}{Pipeline completo de desarrollo}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Fases del desarrollo:}
        \begin{enumerate}
            \item \textbf{Adquisición} de datos
            \item \textbf{Limpieza} y validación
            \item \textbf{Análisis} espacial
            \item \textbf{Visualización}
            \item \textbf{Deployment}
        \end{enumerate}
        
        \vspace{0.3cm}
        \textbf{Stack tecnológico típico:}
        \begin{itemize}
            \item Python/R para análisis
            \item PostGIS para almacenamiento
            \item QGIS para exploración
            \item Web frameworks para deployment
        \end{itemize}
        
        \column{0.5\textwidth}
        \begin{center}
        \begin{tikzpicture}[scale=0.8]
            % Pipeline flow
            \node[draw,rectangle,fill=blue!20] (data) at (0,4) {Datos crudos};
            \node[draw,rectangle,fill=green!20] (clean) at (0,3) {Limpieza};
            \node[draw,rectangle,fill=yellow!20] (analysis) at (0,2) {Análisis};
            \node[draw,rectangle,fill=orange!20] (viz) at (0,1) {Visualización};
            \node[draw,rectangle,fill=red!20] (deploy) at (0,0) {Producción};
            
            \draw[thick,->,>=stealth] (data) -- (clean);
            \draw[thick,->,>=stealth] (clean) -- (analysis);
            \draw[thick,->,>=stealth] (analysis) -- (viz);
            \draw[thick,->,>=stealth] (viz) -- (deploy);
            
            % Feedback loops
            \draw[dashed,->,>=stealth] (analysis.east) .. controls (2,2) and (2,3) .. (clean.east);
            \draw[dashed,->,>=stealth] (viz.east) .. controls (3,1) and (3,2) .. (analysis.east);
        \end{tikzpicture}
        \end{center}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Estructura de proyecto geoespacial}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Organización recomendada:}
        \begin{lstlisting}[language=bash]
proyecto_geo/
├── data/
│   ├── raw/           # Datos originales
│   ├── processed/     # Datos limpios
│   └── cache/         # Resultados cache
├── src/
│   ├── etl/          # Extract-Transform-Load
│   ├── analysis/     # Análisis espacial
│   ├── api/          # REST API
│   └── visualization/ # Mapas y gráficos
├── notebooks/        # Jupyter notebooks
├── tests/           # Tests unitarios
├── config/          # Configuración
├── docker/          # Contenedores
└── docs/           # Documentación
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Archivo de configuración:}
        \begin{lstlisting}[language=Python]
# config.yaml
database:
  host: localhost
  port: 5432
  name: geodatabase
  user: ${DB_USER}
  password: ${DB_PASS}

apis:
  osm_overpass: 
    url: "https://overpass-api.de"
    timeout: 30
  google_maps:
    key: ${GOOGLE_API_KEY}
    
cache:
  enabled: true
  ttl: 3600  # segundos
  
crs:
  default: "EPSG:4326"
  local: "EPSG:32719"  # UTM 19S
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Control de versiones para datos espaciales}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Git + DVC (Data Version Control):}
        \begin{lstlisting}[language=bash]
# Instalar DVC
pip install dvc

# Inicializar DVC en proyecto Git
dvc init

# Agregar archivo grande
dvc add data/comunas_chile.gpkg

# Commit cambios
git add data/comunas_chile.gpkg.dvc
git commit -m "Add comunas dataset"

# Push a storage remoto (S3, GCS, etc)
dvc remote add -d myremote s3://bucket/path
dvc push
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Mejores prácticas:}
        \begin{itemize}
            \item No versionar datos > 100MB en Git
            \item Usar `.gitignore` para datos locales
            \item Documentar origen y fecha de datos
            \item Mantener checksums de archivos
        \end{itemize}
        
        \vspace{0.3cm}
        \textbf{.gitignore típico:}
        \begin{lstlisting}[language=bash]
# Datos
data/raw/*
data/cache/*
*.gpkg
*.tif

# Temporales
*.qgz~
.ipynb_checkpoints/

# Credenciales
.env
config/secrets.yaml
        \end{lstlisting}
    \end{columns}
\end{frame}

\section{Conexión a fuentes de datos reales}

\begin{frame}[fragile]{APIs geoespaciales: OpenStreetMap}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{OSMnx - Python:}
        \begin{lstlisting}[language=Python]
import osmnx as ox
import geopandas as gpd

# Configurar OSMnx
ox.config(use_cache=True, log_console=True)

# Obtener red vial de Santiago
place = "Santiago, Chile"
G = ox.graph_from_place(place, 
                        network_type='drive')

# Convertir a GeoDataFrame
nodes, edges = ox.graph_to_gdfs(G)

# Obtener edificios
buildings = ox.geometries_from_place(
    place, 
    tags={'building': True}
)

# Obtener amenities específicos
tags = {'amenity': ['hospital', 'school']}
amenities = ox.geometries_from_place(
    place, tags
)
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Overpass API - Query directa:}
        \begin{lstlisting}[language=Python]
import requests
import geopandas as gpd

# Query Overpass QL
query = """
[out:json][timeout:25];
area["name"="Santiago"]->.searchArea;
(
  node["amenity"="hospital"](area.searchArea);
  way["amenity"="hospital"](area.searchArea);
);
out geom;
"""

# Ejecutar query
overpass_url = "http://overpass-api.de/api/interpreter"
response = requests.get(
    overpass_url, 
    params={'data': query}
)

data = response.json()
# Convertir a GeoDataFrame...
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{PostGIS: Base de datos espacial}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Conexión y consultas:}
        \begin{lstlisting}[language=Python]
from sqlalchemy import create_engine
import geopandas as gpd
import pandas as pd

# Crear conexión
engine = create_engine(
    'postgresql://user:pass@localhost/geodata'
)

# Leer tabla espacial
sql = """
SELECT 
    c.nombre,
    c.poblacion,
    c.geom,
    COUNT(h.id) as num_hospitales
FROM 
    comunas c
LEFT JOIN 
    hospitales h ON ST_Contains(c.geom, h.geom)
GROUP BY 
    c.id, c.nombre, c.poblacion, c.geom
"""

gdf = gpd.read_postgis(sql, engine, 
                       geom_col='geom')
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Operaciones espaciales en DB:}
        \begin{lstlisting}[language=SQL]
-- Crear índice espacial
CREATE INDEX idx_comunas_geom 
ON comunas USING GIST(geom);

-- Buffer de 1km alrededor de metro
CREATE TABLE areas_metro AS
SELECT 
    id,
    nombre,
    ST_Buffer(geom::geography, 1000)::geometry as buffer
FROM estaciones_metro;

-- Encontrar comunas vecinas
SELECT 
    a.nombre as comuna,
    b.nombre as vecina
FROM 
    comunas a, comunas b
WHERE 
    ST_Touches(a.geom, b.geom)
    AND a.id < b.id;
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{APIs de datos gubernamentales}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{IDE Chile - WFS:}
        \begin{lstlisting}[language=Python]
import geopandas as gpd
from owslib.wfs import WebFeatureService

# Conectar a IDE Chile
wfs_url = "http://www.ide.cl/geoserver/wfs"
wfs = WebFeatureService(url=wfs_url, version='2.0.0')

# Listar capas disponibles
list(wfs.contents)

# Obtener división administrativa
response = wfs.getfeature(
    typename='division_politica:comunas',
    bbox=(-71, -34, -70, -33),  # RM
    srsname='EPSG:4326',
    outputFormat='json'
)

# Leer como GeoDataFrame
import json
comunas = gpd.GeoDataFrame.from_features(
    json.loads(response.read())
)
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{APIs REST - SII, INE:}
        \begin{lstlisting}[language=Python]
# Datos INE - Censo
import requests
import pandas as pd

# API INE (ejemplo)
url = "https://api.ine.cl/datos/censo2017"
params = {
    'region': '13',
    'indicador': 'poblacion',
    'formato': 'json'
}
response = requests.get(url, params=params)
censo_data = pd.DataFrame(response.json())

# Geocodificar direcciones SII
from geopy.geocoders import Nominatim

geolocator = Nominatim(user_agent="my_app")

def geocode_address(address):
    try:
        location = geolocator.geocode(
            f"{address}, Santiago, Chile"
        )
        return location.latitude, location.longitude
    except:
        return None, None
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Google Maps y otras APIs comerciales}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Google Maps Platform:}
        \begin{lstlisting}[language=Python]
import googlemaps
from datetime import datetime

# Cliente con API key
gmaps = googlemaps.Client(key='YOUR_API_KEY')

# Geocoding
geocode_result = gmaps.geocode(
    'Av. Libertador Bernardo O\'Higgins 3363, Santiago'
)

# Places API - buscar hospitales cercanos
places_result = gmaps.places_nearby(
    location=(-33.45, -70.65),
    radius=2000,
    type='hospital'
)

# Distance Matrix - tiempos de viaje
origins = [(-33.45, -70.65), (-33.44, -70.64)]
destinations = [(-33.46, -70.66)]

matrix = gmaps.distance_matrix(
    origins, 
    destinations,
    mode="driving",
    departure_time=datetime.now()
)
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Mapbox - Isócronas:}
        \begin{lstlisting}[language=Python]
import requests

# Isochrone API
url = "https://api.mapbox.com/isochrone/v1/mapbox/driving"

params = {
    'coordinates': '-70.65,-33.45',
    'contours_minutes': '5,10,15',
    'polygons': 'true',
    'access_token': 'YOUR_TOKEN'
}

response = requests.get(url, params=params)
isochrones = response.json()

# Convertir a GeoDataFrame
import geopandas as gpd
from shapely.geometry import shape

features = []
for feature in isochrones['features']:
    geom = shape(feature['geometry'])
    props = feature['properties']
    features.append({'geometry': geom, 
                    'minutes': props['contour']})

gdf = gpd.GeoDataFrame(features)
        \end{lstlisting}
    \end{columns}
\end{frame}

\section{Análisis espacial aplicado}

\begin{frame}[fragile]{Geocodificación masiva}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Pipeline de geocodificación:}
        \begin{lstlisting}[language=Python]
import pandas as pd
import geopandas as gpd
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
import time

# Cargar direcciones
df = pd.read_csv('direcciones.csv')

# Configurar geocoder con rate limiting
geolocator = Nominatim(user_agent="myapp")
geocode = RateLimiter(geolocator.geocode, 
                      min_delay_seconds=1)

# Función robusta de geocodificación
def geocode_df(df, address_col):
    locations = []
    for idx, row in df.iterrows():
        try:
            location = geocode(row[address_col])
            if location:
                locations.append({
                    'lat': location.latitude,
                    'lon': location.longitude,
                    'address': row[address_col]
                })
        except Exception as e:
            print(f"Error en {row[address_col]}: {e}")
            locations.append({'lat': None, 'lon': None})
    return pd.DataFrame(locations)
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Validación y corrección:}
        \begin{lstlisting}[language=Python]
# Validar resultados
def validate_geocoding(gdf, comuna_bounds):
    """Verificar que puntos estén en comuna"""
    valid = []
    for idx, row in gdf.iterrows():
        point = row.geometry
        comuna = comuna_bounds[
            comuna_bounds.contains(point)
        ]
        if len(comuna) > 0:
            valid.append(True)
        else:
            valid.append(False)
    return valid

# Geocoding inverso para verificar
from geopy.geocoders import reverse

def verify_address(lat, lon, expected_comuna):
    location = geolocator.reverse(f"{lat}, {lon}")
    address = location.raw['address']
    
    if expected_comuna.lower() in 
       address.get('city', '').lower():
        return True
    return False
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Análisis de áreas de influencia}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Buffer vs Isócronas reales:}
        \begin{lstlisting}[language=Python]
import osmnx as ox
import networkx as nx
import geopandas as gpd
from shapely.geometry import Point

# Red vial
G = ox.graph_from_place('Santiago, Chile', 
                        network_type='drive')
G = ox.project_graph(G)

# Punto de interés (ej: hospital)
hospital = Point(-70.65, -33.45)
hospital_proj = ox.project_gdf(
    gpd.GeoDataFrame([1], geometry=[hospital], 
                    crs='EPSG:4326')
).geometry[0]

# Nodo más cercano
hospital_node = ox.nearest_nodes(
    G, hospital_proj.x, hospital_proj.y
)

# Isócrona de 10 minutos
travel_speed = 40  # km/h
trip_time = 10 * 60  # 10 min en segundos
meters = travel_speed * 1000 / 60 / 60 * trip_time

# Subgrafo alcanzable
subgraph = nx.ego_graph(G, hospital_node, 
                        radius=meters, 
                        distance='length')
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Análisis de accesibilidad:}
        \begin{lstlisting}[language=Python]
# Calcular accesibilidad a servicios
def calculate_accessibility(puntos, servicios, 
                           max_distance=2000):
    """
    Calcula métricas de accesibilidad
    """
    results = []
    
    for idx, punto in puntos.iterrows():
        # Distancia al servicio más cercano
        distances = servicios.distance(punto.geometry)
        min_dist = distances.min()
        
        # Número de servicios en radio
        within = distances <= max_distance
        count = within.sum()
        
        # Índice de accesibilidad
        if min_dist > 0:
            accessibility = count / (min_dist / 1000)
        else:
            accessibility = count * 10
            
        results.append({
            'id': idx,
            'min_distance': min_dist,
            'services_count': count,
            'accessibility_index': accessibility
        })
    
    return pd.DataFrame(results)
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Clustering espacial}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{DBSCAN espacial:}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import DBSCAN
import numpy as np
import geopandas as gpd

# Preparar datos
coords = np.array([[p.x, p.y] for p in gdf.geometry])

# DBSCAN con distancia en metros
kms_per_radian = 6371.0088
epsilon = 0.5 / kms_per_radian  # 500 metros

db = DBSCAN(eps=epsilon, min_samples=5, 
           algorithm='ball_tree', 
           metric='haversine').fit(np.radians(coords))

# Asignar clusters
gdf['cluster'] = db.labels_

# Análisis de clusters
cluster_stats = gdf.groupby('cluster').agg({
    'precio': ['mean', 'std', 'count'],
    'superficie': 'mean',
    'geometry': lambda x: x.unary_union.centroid
})

# Visualizar hotspots
hotspots = gdf[gdf['cluster'] != -1]
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{K-means ponderado:}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans

# Preparar features espaciales y atributos
X = np.column_stack([
    coords,  # ubicación
    gdf['precio'].values / 1e6,  # normalizado
    gdf['m2'].values / 100
])

# Ponderar componentes
weights = [1.0, 1.0, 0.5, 0.3]  # x,y,precio,m2
X_weighted = X * weights

# K-means
kmeans = KMeans(n_clusters=5, random_state=42)
gdf['segment'] = kmeans.fit_predict(X_weighted)

# Centros de clusters
centers = kmeans.cluster_centers_ / weights
center_points = [Point(c[0], c[1]) for c in centers]

# Polígonos de Voronoi para áreas de mercado
from scipy.spatial import Voronoi
vor = Voronoi(centers[:, :2])
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Interpolación espacial}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Kriging para valores continuos:}
        \begin{lstlisting}[language=Python]
from pykrige.ok import OrdinaryKriging
import numpy as np

# Datos de entrada
points = gdf[['x', 'y', 'precio_m2']].values
lons = points[:, 0]
lats = points[:, 1]
values = points[:, 2]

# Crear grid de interpolación
grid_lon = np.linspace(lons.min(), lons.max(), 100)
grid_lat = np.linspace(lats.min(), lats.max(), 100)

# Ordinary Kriging
OK = OrdinaryKriging(lons, lats, values, 
                     variogram_model='spherical',
                     verbose=False, 
                     enable_plotting=False)

z, ss = OK.execute('grid', grid_lon, grid_lat)

# Convertir a raster
import rasterio
from rasterio.transform import from_origin

transform = from_origin(west, north, pixel_size, pixel_size)
with rasterio.open('interpolated.tif', 'w',
                  driver='GTiff', height=z.shape[0],
                  width=z.shape[1], count=1, 
                  dtype=z.dtype, crs='EPSG:4326',
                  transform=transform) as dst:
    dst.write(z, 1)
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{IDW (Inverse Distance Weight):}
        \begin{lstlisting}[language=Python]
def idw_interpolation(points, values, 
                     grid_points, power=2):
    """
    Interpolación IDW simple
    """
    interpolated = []
    
    for grid_point in grid_points:
        # Distancias a todos los puntos
        distances = np.sqrt(
            (points[:, 0] - grid_point[0])**2 + 
            (points[:, 1] - grid_point[1])**2
        )
        
        # Evitar división por cero
        distances[distances == 0] = 1e-10
        
        # Pesos inversos
        weights = 1 / distances**power
        weights /= weights.sum()
        
        # Valor interpolado
        value = np.sum(weights * values)
        interpolated.append(value)
    
    return np.array(interpolated)

# Aplicar a grid regular
grid_x, grid_y = np.meshgrid(
    np.linspace(min_x, max_x, 50),
    np.linspace(min_y, max_y, 50)
)
        \end{lstlisting}
    \end{columns}
\end{frame}

\section{Optimización y escalabilidad}

\begin{frame}[fragile]{Manejo eficiente de grandes datasets}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Chunking y procesamiento por lotes:}
        \begin{lstlisting}[language=Python]
import geopandas as gpd
import pandas as pd
from shapely import wkt

# Leer en chunks
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('huge_dataset.csv', 
                         chunksize=chunk_size):
    # Procesar chunk
    chunk['geometry'] = chunk['wkt'].apply(wkt.loads)
    gdf_chunk = gpd.GeoDataFrame(chunk, 
                                 crs='EPSG:4326')
    
    # Operación espacial en chunk
    gdf_chunk = gdf_chunk.to_crs('EPSG:32719')
    gdf_chunk['area'] = gdf_chunk.area
    
    # Filtrar y guardar resultado
    filtered = gdf_chunk[gdf_chunk['area'] > 1000]
    chunks.append(filtered)

# Combinar resultados
result = pd.concat(chunks, ignore_index=True)

# Guardar en formato eficiente
result.to_parquet('processed_data.parquet')
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Dask para paralelización:}
        \begin{lstlisting}[language=Python]
import dask_geopandas as dgpd
import dask.dataframe as dd

# Leer dataset particionado
ddf = dgpd.read_parquet(
    'huge_dataset.parquet',
    npartitions=8
)

# Operaciones lazy (no se ejecutan aún)
ddf = ddf.to_crs('EPSG:32719')
ddf['buffer_100m'] = ddf.buffer(100)

# Spatial join paralelo
other_ddf = dgpd.read_file('polygons.gpkg', 
                          npartitions=4)
joined = dgpd.sjoin(ddf, other_ddf, 
                    how='inner', 
                    predicate='intersects')

# Ejecutar y obtener resultado
with dask.config.set(scheduler='threads'):
    result = joined.compute()

# O guardar sin cargar en memoria
joined.to_parquet('joined_results/')
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Índices espaciales y caché}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{R-tree para búsquedas rápidas:}
        \begin{lstlisting}[language=Python]
from rtree import index
import pickle
import hashlib

class SpatialCache:
    def __init__(self):
        self.idx = index.Index()
        self.cache = {}
        
    def add_features(self, features_gdf):
        """Agregar features al índice"""
        for idx, row in features_gdf.iterrows():
            bounds = row.geometry.bounds
            self.idx.insert(idx, bounds)
            self.cache[idx] = row
    
    def query_area(self, bbox):
        """Búsqueda rápida por bbox"""
        candidates = list(self.idx.intersection(bbox))
        return [self.cache[i] for i in candidates]
    
    def nearest(self, point, n=5):
        """N vecinos más cercanos"""
        coords = (point.x, point.y, point.x, point.y)
        nearest = list(self.idx.nearest(coords, n))
        return [self.cache[i] for i in nearest]
    
    def save(self, filename):
        """Persistir caché"""
        with open(filename, 'wb') as f:
            pickle.dump(self.cache, f)
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Memoization de operaciones costosas:}
        \begin{lstlisting}[language=Python]
from functools import lru_cache
import hashlib

def geometry_hash(geom):
    """Hash único para geometría"""
    wkb = geom.wkb
    return hashlib.md5(wkb).hexdigest()

@lru_cache(maxsize=1000)
def expensive_buffer_operation(geom_hash, distance):
    """Operación costosa con caché"""
    # Reconstruir geometría del hash
    geom = cache_dict[geom_hash]
    
    # Operación costosa
    result = geom.buffer(distance)
    for i in range(10):
        result = result.simplify(0.01)
        result = result.buffer(-distance/20)
        result = result.buffer(distance/20)
    
    return result

# Uso con caché
geom_id = geometry_hash(polygon)
result = expensive_buffer_operation(geom_id, 100)
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Optimización de consultas PostGIS}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Índices y particionamiento:}
        \begin{lstlisting}[language=SQL]
-- Índices espaciales y de atributos
CREATE INDEX idx_spatial ON propiedades 
USING GIST(geom);

CREATE INDEX idx_precio ON propiedades(precio);
CREATE INDEX idx_comuna ON propiedades(comuna_id);

-- Índice compuesto
CREATE INDEX idx_spatial_price ON propiedades 
USING GIST(geom, precio_range);

-- Particionamiento por región
CREATE TABLE propiedades_rm PARTITION OF propiedades
FOR VALUES IN (13);

CREATE TABLE propiedades_v PARTITION OF propiedades
FOR VALUES IN (5);

-- Clustering espacial para mejor performance
CLUSTER propiedades USING idx_spatial;

-- Materializar vistas complejas
CREATE MATERIALIZED VIEW mv_stats_comuna AS
SELECT 
    c.id, c.nombre,
    COUNT(p.id) as total_propiedades,
    AVG(p.precio) as precio_promedio,
    ST_Union(p.geom) as coverage
FROM comunas c
LEFT JOIN propiedades p ON ST_Contains(c.geom, p.geom)
GROUP BY c.id, c.nombre;
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{Query optimization:}
        \begin{lstlisting}[language=SQL]
-- Usar ST_DWithin en vez de buffer
-- MALO:
SELECT * FROM points 
WHERE ST_Intersects(
    geom, 
    ST_Buffer(target_point, 1000)
);

-- BUENO:
SELECT * FROM points 
WHERE ST_DWithin(
    geom, 
    target_point, 
    1000
);

-- Simplificar para visualización
SELECT 
    id,
    nombre,
    ST_SimplifyPreserveTopology(
        geom, 
        10  -- tolerancia
    ) as geom_simple
FROM comunas;

-- Usar && para pre-filtrar
SELECT * FROM a, b
WHERE a.geom && b.geom  -- bbox check
AND ST_Intersects(a.geom, b.geom);
        \end{lstlisting}
    \end{columns}
\end{frame}

\section{Deployment de soluciones}

\begin{frame}[fragile]{API REST con FastAPI}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{API geoespacial:}
        \begin{lstlisting}[language=Python]
from fastapi import FastAPI, Query
from pydantic import BaseModel
import geopandas as gpd
from shapely.geometry import Point
import json

app = FastAPI(title="GeoAPI")

# Cache de datos
comunas_gdf = gpd.read_file("comunas.gpkg")

class LocationRequest(BaseModel):
    lat: float
    lon: float

@app.get("/api/comuna")
async def get_comuna(lat: float, lon: float):
    """Obtener comuna de un punto"""
    point = Point(lon, lat)
    
    for idx, comuna in comunas_gdf.iterrows():
        if comuna.geometry.contains(point):
            return {
                "comuna": comuna['nombre'],
                "region": comuna['region'],
                "poblacion": int(comuna['poblacion'])
            }
    
    return {"error": "Punto fuera de Chile"}
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \begin{lstlisting}[language=Python]
@app.get("/api/nearest")
async def nearest_services(
    lat: float, 
    lon: float, 
    service_type: str,
    limit: int = 5
):
    """Servicios más cercanos"""
    point = Point(lon, lat)
    
    services = load_services(service_type)
    services['distance'] = services.distance(point)
    nearest = services.nsmallest(limit, 'distance')
    
    return {
        "type": service_type,
        "results": [
            {
                "name": row['name'],
                "distance": round(row['distance'], 2),
                "address": row['address']
            } 
            for _, row in nearest.iterrows()
        ]
    }

# Ejecutar con: uvicorn main:app --reload
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Dashboard con Streamlit}
    \begin{columns}[T]
        \column{0.58\textwidth}
        \textbf{App interactiva:}
        \begin{lstlisting}[language=Python]
import streamlit as st
import geopandas as gpd
import folium
from streamlit_folium import st_folium

st.set_page_config(page_title="GeoAnalytics", 
                  layout="wide")

st.title("Dashboard Geoespacial")

# Sidebar para controles
with st.sidebar:
    st.header("Filtros")
    
    comuna = st.selectbox("Comuna", 
                         comunas_gdf['nombre'].unique())
    
    precio_min, precio_max = st.slider(
        "Rango de precio",
        0, 1000000000,
        (100000000, 500000000)
    )
    
    year = st.slider("Año", 2020, 2024, 2024)

# Filtrar datos
filtered = propiedades[
    (propiedades['comuna'] == comuna) &
    (propiedades['precio'].between(precio_min, precio_max)) &
    (propiedades['year'] == year)
]
        \end{lstlisting}
        
        \column{0.42\textwidth}
        \begin{lstlisting}[language=Python]
# Métricas
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Total propiedades", 
             len(filtered))
with col2:
    st.metric("Precio promedio", 
             f"${filtered['precio'].mean():,.0f}")
with col3:
    st.metric("M2 promedio", 
             f"{filtered['m2'].mean():.1f}")

# Mapa interactivo
m = folium.Map(location=[-33.45, -70.65], 
              zoom_start=11)

for idx, row in filtered.iterrows():
    folium.CircleMarker(
        [row['lat'], row['lon']],
        radius=5,
        popup=f"${row['precio']:,.0f}",
        color='red',
        fill=True
    ).add_to(m)

st_folium(m, width=700, height=450)
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Docker para aplicaciones geo}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{Dockerfile multi-stage:}
        \begin{lstlisting}[language=Docker]
# Stage 1: Build dependencies
FROM python:3.9-slim as builder

RUN apt-get update && apt-get install -y \
    gdal-bin \
    libgdal-dev \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY requirements.txt .

ENV GDAL_CONFIG=/usr/bin/gdal-config
RUN pip install --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.9-slim

RUN apt-get update && apt-get install -y \
    gdal-bin \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY --from=builder /usr/local/lib/python3.9/site-packages \
                   /usr/local/lib/python3.9/site-packages

COPY . .

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0"]
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{docker-compose.yml:}
        \begin{lstlisting}[language=yaml]
version: '3.8'

services:
  postgis:
    image: postgis/postgis:14-3.2
    environment:
      POSTGRES_DB: geodata
      POSTGRES_USER: geouser
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
  
  api:
    build: .
    environment:
      DB_HOST: postgis
      DB_NAME: geodata
      DB_USER: geouser
      DB_PASSWORD: ${DB_PASSWORD}
    ports:
      - "8000:8000"
    depends_on:
      - postgis
    volumes:
      - ./data:/app/data

volumes:
  pgdata:
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Deployment en la nube}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \textbf{AWS - Terraform:}
        \begin{lstlisting}[language=HCL]
# RDS PostGIS
resource "aws_db_instance" "postgis" {
  engine         = "postgres"
  engine_version = "14.6"
  instance_class = "db.t3.micro"
  
  allocated_storage = 20
  storage_type     = "gp3"
  
  db_name  = "geodata"
  username = "geouser"
  password = var.db_password
  
  # Habilitar PostGIS
  enabled_cloudwatch_logs_exports = ["postgresql"]
}

# Lambda para procesamiento
resource "aws_lambda_function" "geo_processor" {
  function_name = "geo-processor"
  runtime      = "python3.9"
  handler      = "handler.main"
  
  layers = [
    "arn:aws:lambda:region:770693421928:layer:Klayers-p39-gdal:1"
  ]
  
  environment {
    variables = {
      DB_HOST = aws_db_instance.postgis.endpoint
    }
  }
}
        \end{lstlisting}
        
        \column{0.45\textwidth}
        \textbf{GitHub Actions CI/CD:}
        \begin{lstlisting}[language=yaml]
name: Deploy

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Setup Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install GDAL
      run: |
        sudo apt-get update
        sudo apt-get install -y gdal-bin
    
    - name: Test
      run: |
        pip install -r requirements.txt
        pytest tests/
  
  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to server
      run: |
        ssh user@server "cd /app && 
                        git pull && 
                        docker-compose up -d"
        \end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}{Caso práctico: Sistema de valoración inmobiliaria}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Arquitectura completa:}
        \begin{enumerate}
            \item \textbf{Ingesta}: APIs (SII, OSM, Portal Inmobiliario)
            \item \textbf{Storage}: PostGIS + S3 para imágenes
            \item \textbf{Processing}: Airflow para ETL diario
            \item \textbf{Analytics}: Jupyter Hub para DS
            \item \textbf{API}: FastAPI con Redis cache
            \item \textbf{Frontend}: React + Mapbox GL
        \end{enumerate}
        
        \vspace{0.3cm}
        \textbf{Features espaciales:}
        \begin{itemize}
            \item Distancia a metro/paraderos
            \item Densidad de servicios
            \item Índice de vegetación (NDVI)
            \item Contaminación acústica
            \item Riesgo de inundación
        \end{itemize}
        
        \column{0.5\textwidth}
        \begin{center}
        \begin{tikzpicture}[scale=0.65]
            % Data sources
            \node[draw,rectangle,fill=blue!20] (apis) at (0,5) {APIs externas};
            \node[draw,rectangle,fill=blue!20] (scraping) at (2.5,5) {Web scraping};
            
            % ETL
            \node[draw,rectangle,fill=green!20] (airflow) at (1.25,4) {Airflow ETL};
            
            % Storage
            \node[draw,rectangle,fill=yellow!20] (postgis) at (0,3) {PostGIS};
            \node[draw,rectangle,fill=yellow!20] (s3) at (2.5,3) {S3/MinIO};
            
            % Processing
            \node[draw,rectangle,fill=orange!20] (ml) at (1.25,2) {ML Pipeline};
            
            % API
            \node[draw,rectangle,fill=red!20] (api) at (0,1) {REST API};
            \node[draw,rectangle,fill=red!20] (cache) at (2.5,1) {Redis Cache};
            
            % Frontend
            \node[draw,rectangle,fill=purple!20] (web) at (1.25,0) {Web App};
            
            % Connections
            \draw[->] (apis) -- (airflow);
            \draw[->] (scraping) -- (airflow);
            \draw[->] (airflow) -- (postgis);
            \draw[->] (airflow) -- (s3);
            \draw[->] (postgis) -- (ml);
            \draw[->] (ml) -- (api);
            \draw[->] (api) -- (cache);
            \draw[->] (cache) -- (web);
            \draw[->] (api) -- (web);
        \end{tikzpicture}
        \end{center}
    \end{columns}
\end{frame}

\begin{frame}{Mejores prácticas y recomendaciones}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Desarrollo:}
        \begin{itemize}
            \item Usar ambientes virtuales (venv, conda)
            \item Documentar dependencias espaciales
            \item Tests con datos sintéticos
            \item Validar geometrías siempre
            \item Logging detallado de operaciones
        \end{itemize}
        
        \vspace{0.3cm}
        \textbf{Performance:}
        \begin{itemize}
            \item Simplificar geometrías para web
            \item Usar tiles vectoriales para mapas
            \item Cachear resultados costosos
            \item Índices espaciales SIEMPRE
            \item Proyecciones locales para cálculos
        \end{itemize}
        
        \column{0.5\textwidth}
        \textbf{Producción:}
        \begin{itemize}
            \item Monitoreo de queries lentas
            \item Backups de datos espaciales
            \item Rate limiting en APIs
            \item CDN para tiles de mapas
            \item Healthchecks de servicios geo
        \end{itemize}
        
        \vspace{0.3cm}
        \begin{tcolorbox}[colframe=usachred,colback=red!5]
        \textbf{Errores comunes:}
        \begin{itemize}
            \item Mezclar CRS sin transformar
            \item No validar geometrías
            \item Ignorar índices espaciales
            \item Cargar todo en memoria
        \end{itemize}
        \end{tcolorbox}
    \end{columns}
\end{frame}

\begin{frame}{Recursos para proyectos}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Datos Chile:}
        \begin{itemize}
            \item \href{https://www.ide.cl}{IDE Chile}
            \item \href{https://www.bcn.cl/siit}{SIIT - BCN}
            \item \href{https://datos.gob.cl}{Datos.gob.cl}
            \item \href{http://datos.cedeus.cl}{CEDEUS}
            \item \href{https://www.ine.cl}{INE - Censo y cartografía}
        \end{itemize}
        
        \vspace{0.3cm}
        \textbf{Herramientas recomendadas:}
        \begin{itemize}
            \item QGIS para exploración
            \item DBeaver para PostGIS
            \item Jupyter para prototipado
            \item Postman para probar APIs
            \item Docker para desarrollo
        \end{itemize}
        
        \column{0.5\textwidth}
        \textbf{Templates y boilerplates:}
        \begin{itemize}
            \item \href{https://github.com/geopandas/geopandas}{GeoPandas examples}
            \item \href{https://github.com/Toblerity/Fiona}{Fiona recipes}
            \item \href{https://github.com/Leaflet/Leaflet}{Leaflet demos}
            \item \href{https://github.com/uber/kepler.gl}{Kepler.gl}
        \end{itemize}
        
        \vspace{0.3cm}
        \textbf{Documentación esencial:}
        \begin{itemize}
            \item PostGIS.net
            \item GeoPandas.org
            \item OSMnx documentation
            \item Shapely manual
            \item GDAL/OGR cookbook
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}{Cierre}
    \begin{center}
        \Large{¿Preguntas?}
        
        \vspace{1cm}
        
        \textbf{Para sus proyectos:}\\
        Enfóquense en el pipeline completo\\
        desde datos hasta visualización
        
        \vspace{1cm}
        
        \textbf{Próxima clase:}\\
        Análisis espacial y geoestadística
        
        \vspace{0.5cm}
        
        \textbf{Tarea:}\\
        Implementar pipeline básico con\\
        datos reales de su proyecto
    \end{center}
\end{frame}

\end{document}